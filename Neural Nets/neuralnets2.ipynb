{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85b65288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def sigmoidderivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "vec_sigmoid = np.vectorize(sigmoid)\n",
    "vec_sigmoidderivative = np.vectorize(sigmoidderivative)\n",
    "\n",
    "def oneHot(n):\n",
    "    zeros = np.zeros(10)\n",
    "    zeros[n] = 1.0 \n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "\n",
    "    def __init__(self, numinputs:int, numoutputs:int, activation=None):\n",
    "\n",
    "        self.numinputs = numinputs\n",
    "        self.numoutputs = numoutputs\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(self.numoutputs, self.numinputs + 1)\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "    \n",
    "        outputs = self.weights @ inputs # this is \\vec{h}\n",
    "\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                outputs = vec_sigmoid(outputs)\n",
    "\n",
    "            case \"Softmax\":\n",
    "                denom = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    denom += math.exp[outputs[i]]\n",
    "                    outputs[i] = math.exp(outputs[i])\n",
    "                outputs = outputs/denom\n",
    "\n",
    "            case \"ReLU\":\n",
    "                outputs = np.maximum(outputs, 0)\n",
    "            \n",
    "            case \"Tanh\":\n",
    "                #numerator: e^x - e^-x\n",
    "                #denom: e^x + e^-x\n",
    "                outputs = np.tanh(outputs)\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "        \n",
    "    def ComputeLocalGradient(self, inputs):\n",
    "        # z is output after activation\n",
    "        # h is output after linear layer\n",
    "        # w are weights\n",
    "        # Need to compute three things:\n",
    "        # dz/dh\n",
    "        # dh/dw\n",
    "        # dh/dx\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "        outputs = self.weights @ inputs\n",
    "\n",
    "\n",
    "        # This part computes dzdh, and has cases for various activation functions\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                dzdh = np.diag(vec_sigmoidderivative(outputs))\n",
    "            case \"Softmax\":\n",
    "                n = len(outputs)\n",
    "                dzdh = np.zeros((n, n))\n",
    "                denom = 0\n",
    "                for i in range(n):\n",
    "                    denom += math.exp(outputs[i])\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j:\n",
    "                            dzdh[i][j] = (denom * math.exp(outputs[i]) - (math.exp(outputs[i])**2))/(denom**2)\n",
    "                        else:\n",
    "                            dzdh[i][j] = -(math.exp(outputs[j]))*(math.exp(outputs[j]))/(denom**2)\n",
    "\n",
    "            case \"ReLU\":\n",
    "                deriv = np.array([1.0 if num > 0 else 0.0 for num in outputs])\n",
    "                dzdh = np.diag(deriv)\n",
    "            case \"Tanh\":\n",
    "                dzdh = np.diag(1- np.tanh(outputs)**2)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # This part computes dhdw        \n",
    "        dhdw = np.zeros((self.numoutputs, self.numoutputs, self.numinputs+1)) #because of bias\n",
    "        for i in range(self.numoutputs):\n",
    "            for j in range(self.numinputs):\n",
    "                dhdw[i,i,j] = inputs[j]\n",
    "            dhdw[i,i,self.numinputs] = 1\n",
    "            \n",
    "        # This part computes dhdx\n",
    "        dhdx = self.weights[:, :-1]\n",
    "\n",
    "\n",
    "        return (dzdh, dhdw, dhdx)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35480923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3, 6)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "Layer1 = NeuralLayer(5,3,\"Sigmoid\")\n",
    "#Layer1.weights = np.array([[1, 2, -1], [3, -2, 1]])\n",
    "test1 = np.array([1, 2,3,4,5])\n",
    "(dzdh, dhdw, dhdx) = Layer1.ComputeLocalGradient(test1)\n",
    "print(dzdh.shape)\n",
    "print(dhdw.shape)\n",
    "print(dhdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, errorfunc=None):\n",
    "        \n",
    "        self.errorfunc = errorfunc\n",
    "        self.layers = []\n",
    "        self.numlayers = 0\n",
    "\n",
    "    def AppendLayer(self, layer: NeuralLayer):\n",
    "        # need to check that the new layer to be appended has same \n",
    "        # number of inputs as the last layer already in the network\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.numinputs == self.layers[-1].numoutputs:\n",
    "                self.layers.append(layer)\n",
    "                self.numlayers += 1\n",
    "            else:\n",
    "                print(\"Error: number of inputs does not match previous layer\")\n",
    "        else:\n",
    "            self.layers.append(layer)\n",
    "            self.numlayers += 1\n",
    "\n",
    "        \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(self.layers[0].Evaluate(inputs))\n",
    "\n",
    "        for i in range(1,self.numlayers):\n",
    "            outputs.append(self.layers[i].Evaluate(outputs[i-1]))\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def ComputeError(self, inputs, trueoutputs):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        \n",
    "        if self.errorfunc == \"MSE\":\n",
    "            n = len(outputs[-1])\n",
    "            diffs = outputs[-1] - trueoutputs\n",
    "            err = np.dot(diffs, diffs)\n",
    "            err = err/(2*n)\n",
    "            return err\n",
    "\n",
    "    def BackPropagate(self, inputs, trueoutputs, learningrate):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        gradients = []\n",
    "\n",
    "        # Compute all the necessary gradients\n",
    "        for i in range(self.numlayers):\n",
    "            if i == 0:\n",
    "                tempinput = inputs\n",
    "            else:\n",
    "                tempinput = outputs[i-1]\n",
    "            \n",
    "            gradients.append(self.layers[i].ComputeLocalGradient(tempinput))\n",
    "\n",
    "        match self.errorfunc:\n",
    "            case \"MSE\":\n",
    "                dldz = (0.5) * (outputs[-1] - trueoutputs)\n",
    "\n",
    "            case \"CrossEntropy\":\n",
    "                dldz = np.zeros(len(trueoutputs))\n",
    "                spot = np.where(1 == trueoutputs)\n",
    "                dldz[spot] = 1/outputs[-1][spot]\n",
    "                \n",
    "            \n",
    "\n",
    "        # Update weights, working backwards\n",
    "\n",
    "        currgrad = dldz @ gradients[-1][0]\n",
    "    \n",
    "        for i in range(self.numlayers-1, -1, -1):\n",
    "            self.layers[i].weights -= learningrate * (currgrad @ gradients[i][1])\n",
    "            currgrad = currgrad @ gradients[i][0] @ gradients[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef2ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211255137791783\n",
      "0.19876774433092836\n",
      "0.18843136732982918\n",
      "0.1771557257708931\n",
      "0.1638492459219142\n",
      "0.14792426256519203\n",
      "0.1294350639364992\n",
      "0.10939777319709791\n",
      "0.08965002703059298\n",
      "0.07213187412895564\n",
      "0.058025151539170976\n"
     ]
    }
   ],
   "source": [
    "MyNN = NeuralNetwork(errorfunc=\"MSE\")\n",
    "MyLayer1 = NeuralLayer(5, 3, \"Sigmoid\")\n",
    "MyLayer2 = NeuralLayer(3, 2, \"Sigmoid\")\n",
    "\n",
    "\n",
    "MyNN.AppendLayer(MyLayer1)\n",
    "MyNN.AppendLayer(MyLayer2)\n",
    "\n",
    "myinput = np.array([1,2,3,4,5])\n",
    "mytrue = np.array([1,0])\n",
    "\n",
    "print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "for i in range(10):\n",
    "    MyNN.BackPropagate(myinput, mytrue, 1)\n",
    "    print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6b2becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST database\n",
    "(x_train0, y_train0), (x_test0, y_test0) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train0.shape == (60000, 28, 28)\n",
    "assert x_test0.shape == (10000, 28, 28)\n",
    "assert y_train0.shape == (60000,)\n",
    "assert y_test0.shape == (10000,)\n",
    "\n",
    "# Prepare data for processing\n",
    "# x_train and x_test need to be reshaped and converted to np.float64\n",
    "# y_train and y_test need to be one-hot encoded\n",
    "x_train = np.zeros((6000, 28*28)) #creates a empty 2d matrix with 6000 rows, and 28*28 columns\n",
    "x_test = np.zeros((1000, 28*28))\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = x_train0[i].reshape(28*28).astype(np.float64) / 255\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = x_test0[i].reshape(28*28).astype(np.float64) / 255\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(6000):\n",
    "    y_train.append(oneHot(y_train0[i]))\n",
    "for i in range(1000):\n",
    "    y_test.append(oneHot(y_test0[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0adc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22827517506054051\n",
      "0.22712407488980474\n",
      "0.22595724260855454\n",
      "0.2247813184038893\n",
      "0.22360362521614766\n",
      "0.22243191994306688\n",
      "0.2212740982464195\n",
      "0.22013787690258296\n",
      "0.219030482399749\n",
      "0.21795837470078983\n",
      "0.21692703048650253\n",
      "0.21594080173075508\n",
      "0.21500285504810698\n",
      "0.21411518713761196\n",
      "0.21327870371542476\n",
      "0.21249334465460032\n",
      "0.21175823676383812\n",
      "0.21107185712385085\n",
      "0.2104321931604472\n",
      "0.20983688963497293\n",
      "0.2092833766508611\n",
      "0.20876897609096154\n",
      "0.20829098638703894\n",
      "0.2078467471705677\n",
      "0.2074336862811697\n",
      "0.20704935198712535\n",
      "0.2066914332746405\n",
      "0.2063577708367342\n",
      "0.20604636105093208\n",
      "0.20575535485475985\n",
      "0.20548305305761874\n",
      "0.20522789929353671\n",
      "0.2049884715331644\n",
      "0.20476347283745291\n",
      "0.2045517218466102\n",
      "0.20435214335045107\n",
      "0.2041637591735344\n",
      "0.20398567952403585\n",
      "0.20381709489329136\n",
      "0.2036572685483582\n",
      "0.20350552962865992\n",
      "0.2033612668364974\n",
      "0.20322392269731265\n",
      "0.20309298835708728\n",
      "0.20296799887960298\n",
      "0.20284852900434336\n",
      "0.20273418932572032\n",
      "0.2026246228554342\n",
      "0.20251950193167895\n",
      "0.20241852544124977\n",
      "0.20232141632318\n",
      "0.2022279193251662\n",
      "0.20213779898663362\n",
      "0.20205083782477948\n",
      "0.20196683470227011\n",
      "0.20188560335743527\n",
      "0.20180697107979748\n",
      "0.2017307775155818\n",
      "0.2016568735894985\n",
      "0.2015851205305637\n",
      "0.2015153889910528\n",
      "0.2014475582488644\n",
      "0.20138151548463554\n",
      "0.2013171551258869\n",
      "0.20125437825132142\n",
      "0.20119309204914212\n",
      "0.20113320932391948\n",
      "0.201074648047127\n",
      "0.20101733094698643\n",
      "0.20096118513372682\n",
      "0.2009061417567753\n",
      "0.20085213569076074\n",
      "0.20079910524753988\n",
      "0.20074699191173892\n",
      "0.20069574009756583\n",
      "0.2006452969248731\n",
      "0.20059561201265433\n",
      "0.20054663728834105\n",
      "0.20049832681142438\n",
      "0.20045063661007143\n",
      "0.20040352452953475\n",
      "0.20035695009126608\n",
      "0.20031087436174935\n",
      "0.20026525983015747\n",
      "0.20022007029402084\n",
      "0.20017527075216596\n",
      "0.20013082730425044\n",
      "0.20008670705627654\n",
      "0.20004287803151857\n",
      "0.19999930908634694\n",
      "0.19995596983047034\n",
      "0.19991283055115913\n",
      "0.19986986214104072\n",
      "0.19982703602909285\n",
      "0.19978432411448196\n",
      "0.19974169870292058\n",
      "0.19969913244523602\n",
      "0.1996565982778622\n",
      "0.19961406936498022\n",
      "0.19957151904204967\n",
      "0.19952892076048087\n",
      "0.19952892076048087\n",
      "Final check evaluation: [array([9.99998252e-01, 1.01891882e-01, 9.14636943e-02, 7.04532178e-05,\n",
      "       3.71264794e-02, 2.27553738e-02, 9.99780074e-01, 1.35979258e-05,\n",
      "       1.32736922e-01, 9.99134435e-01])]\n"
     ]
    }
   ],
   "source": [
    "MyMNISTNetwork = NeuralNetwork(\"MSE\")\n",
    "MyMNISTNetwork.AppendLayer(NeuralLayer(28*28,10,\"Sigmoid\"))\n",
    "\n",
    "\n",
    "y_train0[0]\n",
    "\n",
    "testinput = np.astype(x_train0[0].reshape(28*28), np.float64)\n",
    "testinput /= 255.0\n",
    "#print(testinput.sum())\n",
    "#print(MyMNISTNetwork.Evaluate(testinput))\n",
    "#print(MyMNISTNetwork.layers[-1].weights.dtype)\n",
    "\n",
    "onehot = np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "\n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "for i in range(100):\n",
    "    MyMNISTNetwork.BackPropagate(testinput, onehot, 1)\n",
    "    print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "    \n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "\n",
    "print(\"Final check evaluation: \" + str(MyMNISTNetwork.Evaluate(testinput)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
